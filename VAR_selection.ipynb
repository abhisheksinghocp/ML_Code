{"cells":[{"cell_type":"code","source":["#f_regression: Used only for numeric targets and based on linear regression performance.#\n\n#f_classif: Used only for categorical targets and based on the Analysis of Variance (ANOVA) statistical test.\n\n#chi2: Performs the chi-square statistic for categorical targets, which is less sensible to the nonlinear relationship between the predictive variable and its target.\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import statsmodels.api as sm\nfrom sklearn import datasets\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["data = datasets.load_boston()\nX = pd.DataFrame(data.data,columns=data.feature_names)\ny = pd.DataFrame(data.target, columns=[\"MEDV\"])"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nfrom sklearn import datasets, svm\nfrom sklearn.feature_selection import SelectPercentile, f_classif,f_regression\n# #############################################################################\n# Univariate feature selection with F-test for feature scoring\n# We use the default selection function: the 10% most significant features\nX_indices = np.arange(X.shape[-1])  ## number of column\nselector = SelectPercentile(f_regression, percentile=25) # fro classification SelectPercentile(f_classif, percentile=25)\nselector.fit(X, y)\nscores = -np.log10(selector.pvalues_)\nscores /= scores.max()\nplt.bar(X_indices - .45, scores, width=.2,\n        label=r'Univariate score ($-Log(p_{value})$)', color='darkorange',\n        edgecolor='black')\nplt.show()\nprint(pd.DataFrame(X.columns,selector.pvalues_))\nprint(scores)\n#print(np.argsort(scores))\n\nfor n,s in zip(X.columns,selector.scores_):\n print ('F-score: %3.2ft for feature %s ' % (s,n))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#greedy search RFECV\n\n#Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from sklearn.feature_selection import RFECV\nfrom sklearn.svm import SVR\nestimator = SVR(kernel=\"linear\")\nselector = RFECV(estimator, cv=10,scoring='mean_squared_error')\nselector.fit(X, y)\nprint('Optimal number of features: %d' % selector.n_features_)\nprint (X.columns[selector.support_])"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score MSE\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\nplt.show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#Feature selection using SelectFromModel and LassoCV"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from sklearn.datasets import load_boston\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["X.shape"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\nclf = LassoCV()\n\n# Set a minimum threshold of 0.25\nsfm = SelectFromModel(clf, threshold=0.25)\nsfm.fit(X, y)\nn_features = sfm.transform(X).shape[1]\nprint(n_features)\nprint(sfm.threshold)\n# Reset the threshold till the number of features equals two.\n# Note that the attribute can be set directly instead of repeatedly\n# fitting the metatransformer.\nwhile n_features > 4:\n    sfm.threshold += 0.1\n    X_transform = sfm.transform(X)\n    #print('X_transform',X_transform)\n    n_features = X_transform.shape[1]\n    #print('n_features',n_features)\n\n# Plot the selected two features from X.\nplt.title(\n    \"Features selected from Boston using SelectFromModel with \"\n    \"threshold %0.3f.\" % sfm.threshold)\nfeature1 = X_transform[:, 0]\nfeature2 = X_transform[:, 1]\n\nfeature3 = X_transform[:, 2]\nfeature4 = X_transform[:, 3]\n\n\nplt.plot(feature1, feature2, 'r.')\nplt.plot(feature3, feature4, 'y*')\nplt.xlabel(\"Feature number 1,3\")\nplt.ylabel(\"Feature number 2,4\")\n#plt.ylim([np.min(feature2), np.max(feature2)])\nplt.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["X.shape"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["'''Assessing feature importance via random decision forests\n\n    Let's train a forest of 10,000 trees on the Wine dataset.'''"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier # both option are here"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["X.columns[:]"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["lables = X.columns\nforest = RandomForestRegressor(n_estimators=10000,random_state=0,n_jobs=1)\nforest.fit(X,y)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["imp_feat = forest.feature_importances_\nindicate = np.argsort(imp_feat)[::-1]\n\nfor f in range(X.shape[1]):\n    print('%2d) %-*s %f' %(f + 1,30,lables[f],imp_feat[indicate[f]]))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#PCA"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression,LogisticRegression"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Apply Standarded \nsc = StandardScaler()\nX_STD = sc.fit_transform(X)\n\npca = PCA(n_components=2)\npca.fit(X)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n\nprint(pca.explained_variance_ratio_)  \n#print (pca.singular_values_) in python-3 it will work\n\n\n'''\n# Apply PCA\npca = PCA(n_components=2)\nlr= LinearRegression()\nX_pca = pca.fit_transform(X_STD)\nlr.fit(X_pca,y)\n'''"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["principalDf.head()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["finalDf = pd.concat([principalDf, y], axis=1)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["finalDf.head()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# above steps said 2 var is need but how to know how many var are required , to Final do below steps\npca = PCA()\npca.fit(X)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\nplt.show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n\n# test_size: what proportion of original data is used for test set\ntrain_x, test_x, train_y, test_y = train_test_split( X, y, test_size=1/7.0, random_state=1)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Apply PCA\npca = PCA(n_components=2)\nlr= LinearRegression()\nX_pca_train = pca.fit_transform(train_x)\nmodel=lr.fit(X_pca_train,train_y)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["X_pca_test = pca.fit_transform(test_x)\npredicated_test = model.predict(X_pca_test)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Plot outputs\n#plt.plot(test_y,  color='black')\nplt.plot(predicated_test, color='blue', linewidth=1)\n\n#plt.plot(predication,target)\nplt.show()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":29}],"metadata":{"name":"VAR_selection","notebookId":1392684168635867},"nbformat":4,"nbformat_minor":0}
